{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Simple _**agentic**_ RAG demo\n",
    "\n",
    "Or more precisely: **CRAG** (_Corrective RAG_) - a technique for a little bit smarter RAG\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/tolo/simple-rag-agent-demo/blob/main/simple-rag-agent-demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a><br/>\n",
    "\n",
    "![Corrective RAG](https://github.com/tolo/simple-rag-agent-demo/blob/main/images/crag-flow.png?raw=true)"
   ],
   "id": "4c9af2cedbcec94d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "2fcf534f63f3d850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install httpx~=0.28.1 openai~=1.57 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 lark~=1.2 --upgrade --quiet\n",
    "%pip install langchain~=0.3.10 langchain_openai~=0.2.11 langchain_community~=0.3.10 langchain-chroma~=0.1.4 --upgrade --quiet\n",
    "%pip install langgraph~=0.2.56 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "de7f94673c1cabdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "3d510e51c0909d92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
   ],
   "id": "7085d7c17c7156a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Chat Model",
   "id": "5b5192ae79a33f11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\",temperature=0.0)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ],
   "id": "8f6092d8a5ac3984",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup ingestion / retrieval pipeline",
   "id": "84e945bc7bc2583e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup vector DB (Chroma)",
   "id": "116bb16b204d989b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "persist_directory = './db/chroma/'\n",
    "vectordb: Chroma = Chroma(\n",
    "    collection_name=\"my_index\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory # Optionally persist the database\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ],
   "id": "9d222640ccefee9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup a text splitter",
   "id": "e3a2bb279f787706"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 80\n",
    ")"
   ],
   "id": "72bdfcbd420696d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup documents to load",
   "id": "3ee64e2b269762e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Documents to load (tuple of document_id and document_url)\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DocInfo:\n",
    "    id: str\n",
    "    url: str\n",
    "\n",
    "documents: list[DocInfo] = [\n",
    "    DocInfo(\"1\", \"https://data.riksdagen.se/fil/B9E2F955-31EA-4E9E-91EB-9AE0A3A8FFA7\"), # Förordning om artificiell intelligens, 2020/21:FPM109\n",
    "    DocInfo(\"2\", \"https://data.riksdagen.se/fil/BECC9F0F-3DA1-4F44-9417-02DF027DA29C\"), # VITBOK Om artificiell intelligens - en EU-strategi för spetskompetens och förtroende\n",
    "    DocInfo(\"3\", \"https://data.riksdagen.se/fil/C40BB689-7E23-4593-BDC6-DBEE327C00C6\"), # Risker och möjligheter med artificiell intelligens, 2022/23:374\n",
    "    DocInfo(\"4\", \"https://data.riksdagen.se/fil/4C47740C-D13E-4E22-80CB-43DD1E101080\"), # Direktiv om skadeståndsansvar gällande artificiell intelligens, 2022/23:FPM8\n",
    "]"
   ],
   "id": "b5a1319121a51e71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ingest - split and add to vector index",
   "id": "6c3d3524dfc1ec9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import time\n",
    "\n",
    "def ingest_documents(doc_info: DocInfo):\n",
    "    # Check if document already exists\n",
    "    existing = vectordb.get(where={\"doc_id\": doc_info.id})\n",
    "    if existing[\"documents\"]:\n",
    "        print(f\"Document {doc_info.id} already exists in index\")\n",
    "        return\n",
    "\n",
    "    # Load\n",
    "    print(f\"Loading document {doc_info.id} ({doc_info.url})...\")\n",
    "    loader = PyPDFLoader(doc_info.url)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        page.metadata[\"doc_id\"] = doc_info.id\n",
    "\n",
    "    # Split\n",
    "    doc_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "    # Add to index\n",
    "    print(f\"Adding document {doc_info.id} ({doc_info.url}) to index...\")\n",
    "\n",
    "    # Add in batches, with delay, to avoid rate limiting\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(doc_splits), batch_size):\n",
    "        batch = doc_splits[i:i + batch_size]\n",
    "        vectordb.add_documents(documents=batch)\n",
    "        print(f\"Added splits {i} to {i + batch_size}\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(f\"Added document {doc_info.id} ({doc_info.url}) ({len(pages)} pages) - {len(doc_splits)} splits\")\n",
    "\n",
    "\n",
    "for doc_info in documents:\n",
    "    ingest_documents(doc_info)"
   ],
   "id": "31138108665f383",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup query graph / pipeline",
   "id": "fe1dc3a4f6d9fb17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Graph state",
   "id": "fa83c677f583ec19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import  List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class GraphState(MessagesState):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    irrelevant_docs: bool\n",
    "    answer: str\n"
   ],
   "id": "b9f33014df9438d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nodes",
   "id": "492b3adb4e0bee5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retrieval (Vector Store similarity search)",
   "id": "2a970e8996661772"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RetrievalNode:\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---RETRIEVE---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        # Retrieval\n",
    "        documents = retriever.invoke(question)\n",
    "\n",
    "        print(f\"---RETRIEVED {len(documents)} DOCS---\")\n",
    "        #print(f\"{documents}\")\n",
    "\n",
    "        return {**state, \"documents\": documents}\n",
    "        #return {\"question\": question, \"documents\": documents}"
   ],
   "id": "5370fb2ff04b4f3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retrieval Grader",
   "id": "ae1443e1fe2ebfb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class RetrievalGraderNode:\n",
    "    system_template = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "    Give a binary score '1' or '0' score to indicate whether the document is relevant to the question.\n",
    "\n",
    "    **Retrieved document:** \\n\\n {document}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # Score each doc\n",
    "        filtered_docs = []\n",
    "        irrelevant_docs = True\n",
    "\n",
    "        for (i, d) in enumerate(documents):\n",
    "            grade = self.chain.invoke(\n",
    "                {\"question\": question, \"document\": d.page_content}\n",
    "            )\n",
    "            if \"1\" in grade:\n",
    "                print(f\"---GRADE: DOCUMENT {i} RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "                irrelevant_docs = False\n",
    "            else:\n",
    "                print(f\"---GRADE: DOCUMENT {i} NOT RELEVANT---\")\n",
    "                continue\n",
    "\n",
    "        return {**state, \"documents\": filtered_docs, \"irrelevant_docs\": irrelevant_docs}"
   ],
   "id": "cd2af74da9518cb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Web Search (in case irrelevnat relevant docs were found)\n",
    "\n",
    "As a fallback when there are no/few relevant docs, we can use a web search tool to find more information. In this case, we use the service [Tavily](https://tavily.com)."
   ],
   "id": "120e73d628553e25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "class WebSearchNode:\n",
    "    web_search_tool: TavilySearchResults\n",
    "\n",
    "    def __init__(self):\n",
    "        self.web_search_tool = TavilySearchResults(max_results=8)\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---WEB SEARCH---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        docs = self.web_search_tool.invoke({\"query\": question})\n",
    "        web_results_urls = \"\\n\".join([d[\"url\"] for d in docs])\n",
    "        print(\"---WEB SEARCH RESULT:\")\n",
    "        print(web_results_urls)\n",
    "\n",
    "        web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "        web_results = Document(page_content=web_results)\n",
    "        documents.append(web_results)\n",
    "\n",
    "        return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "# class FakeWebSearchNode:\n",
    "#     system_template = \"\"\"You are a helpful and cheerful assistant.\"\"\"\n",
    "#\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\"system\", system_template),\n",
    "#             (\"human\", \"{question}\"),\n",
    "#         ]\n",
    "#     )\n",
    "#\n",
    "#     chain: Runnable\n",
    "#\n",
    "#     def __init__(self, llm):\n",
    "#         self.chain = self.prompt | llm.bind(temperature=1.0) | StrOutputParser()\n",
    "#\n",
    "#     def __call__(self, state: GraphState):\n",
    "#         print(\"---FAKE WEB SEARCH---\")\n",
    "#         question = state[\"question\"]\n",
    "#\n",
    "#         web_results = self.chain.invoke({\"question\": question})\n",
    "#\n",
    "#         print(f\"---FAKE WEB SEARCH RESULT: \\n{web_results}\")\n",
    "#\n",
    "#         web_results = Document(page_content=web_results)\n",
    "#         documents.append(web_results)\n",
    "#\n",
    "#         return {\"documents\": documents, \"question\": question}"
   ],
   "id": "6e2d6bcf9bf8a226",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### RAG Generation (LLM call with factual/grounded context)",
   "id": "59a1476bcc382c5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RAGNode:\n",
    "    system_template = \"\"\"You are an helpful assistant, expert in answering questions based on provided sources (snippets from documents) and citing the sources used to generate the answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
    "    ALWAYS respond in the SAME language as the original question.\n",
    "\n",
    "    ** Context (snippets from documents): **\n",
    "\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---GENERATE---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # RAG generation - setup context (i.e. relevant documents snippets)\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "        # RAG generation - generate answer\n",
    "        answer = self.chain.invoke({\"question\": question, \"context\": context})\n",
    "        #print(f\"---GENERATE - ANSWER: \\n{answer}\")\n",
    "\n",
    "        return {\"documents\": documents, \"answer\": answer}"
   ],
   "id": "33440df4763d1ef3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conditional edges",
   "id": "b605d784ca5da455"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def decide_to_generate(state):\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    irrelevant_docs: bool = state[\"irrelevant_docs\"]\n",
    "\n",
    "    if irrelevant_docs:\n",
    "        print(\n",
    "            \"---DECISION: USE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"fallback\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ],
   "id": "4c6a592b41a440c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build Graph",
   "id": "ccd3947d8187653a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Graph ####\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", RetrievalNode())  # retrieve\n",
    "workflow.add_node(\"grade_documents\", RetrievalGraderNode())  # grade documents\n",
    "workflow.add_node(\"web_search\", WebSearchNode())  # fallback - web search\n",
    "workflow.add_node(\"generate\", RAGNode())  # generate\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"fallback\": \"web_search\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "#memory = MemorySaver()\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ],
   "id": "b04bc48f05ca2985",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Use Graph",
   "id": "665e36939b57669f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"Vad innebär vitboken om artificiell intelligens?\" # Should NOT result in web search\n",
    "    #\"question\": \"Vilka är nobelpristagarna 2024?\" # Should result in web search\n",
    "    #\"question\": \"Har det i riksdagen diskuterats något om risker kring användningen av artificiell intelligens (AI)?\"\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"thread_id\": \"1\",\n",
    "    #\"max_concurrency\": 1,\n",
    "    #\"recursion_limit\": 5\n",
    "}\n",
    "\n",
    "# Execute graph\n",
    "result = graph.invoke(inputs, {\"configurable\": config})\n",
    "\n",
    "print(f\"--- ANSWER: ---\\n{result['answer']}\")"
   ],
   "id": "e13e461d67c05d10",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
