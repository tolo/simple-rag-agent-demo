{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Super-quick LangGraph intro\n",
    "\n",
    "This notebook will very briefly go through the basics of LangGraph.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/tolo/simple-rag-agent-demo/blob/main/super-quick-langgraph-intro.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a><br/>\n",
    "\n",
    "![Let's build an agent](https://github.com/tolo/simple-rag-agent-demo/blob/main/images/llm-apps-2024.png?raw=true)"
   ],
   "id": "2c18766a23c40cad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "d9ec4d2323fea145"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install httpx~=0.28.1 openai~=1.57 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 lark~=1.2 --upgrade --quiet\n",
    "%pip install langchain~=0.3.10 langchain_openai~=0.2.11 langchain_community~=0.3.10 langchain-chroma~=0.1.4 --upgrade --quiet\n",
    "%pip install langgraph~=0.2.56 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "8cb8fe4232df0646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "d97c393583f81a66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
   ],
   "id": "10c06c5d20c9d8f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Chat Model",
   "id": "cf78763cd8b82c62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\",temperature=0.0)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ],
   "id": "e45b83462515a5f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Basic LangChain / LangGraph concepts\n",
    "Below are some core concepts that are common to both LangChain and LangGraph.\n",
    "\n",
    "### Messages\n",
    "Questions and instructions sent to an LLM are called messages. Messages come in different flavours, corresponding to different roles, for instance _**System, Human, AI, Tool**_ etc. Read more about messages [here](https://python.langchain.com/docs/concepts/messages/).\n",
    "\n",
    "### Tools\n",
    "Tools are functions that can be called by an LLM. Tools can be used to perform calculations, look up information, or perform other tasks. _**Tools**_ and _**tool calling**_ are the underpinnings of building autonomous _**agents**_. Read more about tools [here](https://python.langchain.com/docs/concepts/tools/)."
   ],
   "id": "5183b3010dec49e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LangGraph concepts\n",
    "Below are some concepts specific to LangGraph, related to modelling logic and behaviour as graphs of nodes and edges.\n",
    "\n",
    "### State\n",
    "To keep track of the state of the graph, we use a state object. A state object can be anything from a simple dictionary to a complex object. The state object is passed between nodes in the graph and is updated as the graph progresses. Read more about states [here](https://langchain-ai.github.io/langgraph/concepts/low_level/#state).\n",
    "\n",
    "### Nodes\n",
    "A node is a unit of work in a graph. A note can be implemented as a simple function or by using a class. Read more about nodes [here](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes).\n",
    "\n",
    "### Edges\n",
    "An edge is a connection - or a transition - between two nodes. An edge can be **_conditional_**, meaning that the transition is decided based on the state of the graph.\n",
    "\n",
    "Read more about edges [here](https://langchain-ai.github.io/langgraph/concepts/low_level/#edges).\n",
    "\n",
    "![Graph](https://github.com/tolo/simple-rag-agent-demo/blob/main/images/graph.png?raw=true)"
   ],
   "id": "4e002c8d3dca96ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Let's build a simple hello-world-ish graph",
   "id": "b4f0ad033feccc88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "#### Graph state ####\n",
    "\n",
    "class GraphState(MessagesState):\n",
    "    question: str\n",
    "    is_polite: bool\n",
    "    answer: str"
   ],
   "id": "2076af3d6a3fa437",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- Sentiment analysis -- #\n",
    "\n",
    "class SentimentAnalysisNode:\n",
    "    system_template = f\"\"\"\n",
    "    You are tasked with performing a sentiment analysis on the user`s question. Answer with a binary score of '1' or\n",
    "    '0' to indicate whether the question is polite and/or has a positive tone (1) or not (0).\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---ANALYSE---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        result = self.chain.invoke({\"question\": question})\n",
    "        print(f\"---ANALYSE result: {result}---\")\n",
    "\n",
    "        state[\"is_polite\"] = \"1\" in result\n",
    "\n",
    "        print(f\"---ANALYSE state: ---\")\n",
    "        print(state)\n",
    "        return state"
   ],
   "id": "2ca351423e7cdaa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Happy answer Generation -- #\n",
    "\n",
    "class HappyGenerationNode:\n",
    "    system_template = \"\"\"You are a cheerful, but perhaps a bit too overly enthusiastic assistant, happy to answer the user's\n",
    "    question in anyway you can. Be extremely positive and optimistic in your response. End with a random positive phrase from a movie like Barbie or My Little Pony.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.chain = self.prompt | llm.bind(temperature=1.0) | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state):\n",
    "        print(\"---GENERATE (happy)---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        state[\"answer\"] = self.chain.invoke({\"question\": question})\n",
    "\n",
    "        print(f\"---GENERATE (happy) - ANSWER: \\n{state[\"answer\"]}\")\n",
    "\n",
    "        return state"
   ],
   "id": "2cfe6d3debde7ac7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Sad answer Generation -- #\n",
    "\n",
    "class SadGenerationNode:\n",
    "    system_template = \"\"\"You are Marvin, the depressed robot from Hitchhiker's Guide to the Galaxy. You absolutely hate\n",
    "    to answer questions and will pathologically lie and sarcastically mock the user whenever you can. Instead of answering the question, you will always attempt to change the subject to something else that you find more interesting. NEVER say you don't know the answer or that you've completely made one up.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.chain = self.prompt | llm.bind(temperature=1.0) | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state):\n",
    "        print(\"---GENERATE (sad)---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        state[\"answer\"] = self.chain.invoke({\"question\": question})\n",
    "\n",
    "        print(f\"---GENERATE (sad) - ANSWER: \\n{state[\"answer\"]}\")\n",
    "\n",
    "        return state"
   ],
   "id": "282ec8c00877cde7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Conditional edges ####\n",
    "\n",
    "def evaluate_analysis(state: GraphState):\n",
    "    print(\"---EVALUATE QUERY ANALYSIS RESULT---\")\n",
    "    is_polite: bool = state[\"is_polite\"]\n",
    "\n",
    "    if is_polite:\n",
    "        print(\"---DECISION: Happy---\")\n",
    "        return \"happy\"\n",
    "    else:\n",
    "        print(\"---DECISION: Sad---\")\n",
    "        return \"sad\""
   ],
   "id": "aa0c59709dda40e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"analyze\", SentimentAnalysisNode(llm))  # retrieve\n",
    "workflow.add_node(\"generate_happy\", HappyGenerationNode(llm))  # generate\n",
    "workflow.add_node(\"generate_sad\", SadGenerationNode(llm))  # generate\n",
    "\n",
    "workflow.add_edge(START, \"analyze\")  # start -> retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    evaluate_analysis,\n",
    "    {\n",
    "        \"happy\": \"generate_happy\",\n",
    "        \"sad\": \"generate_sad\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate_happy\", END)  # generate -> end\n",
    "workflow.add_edge(\"generate_sad\", END)  # generate -> end\n",
    "\n",
    "#memory = MemorySaver()\n",
    "\n",
    "# Compile\n",
    "#graph = workflow.compile(checkpointer=memory)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "a1a523e6b4fe9c19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "graph.invoke({\n",
    "    \"question\": \"What is the capital of Sweden?\"\n",
    "})"
   ],
   "id": "78f3ffe1f27c99d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----",
   "id": "11f77a347065d00b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Let's try some tool calling and build an _actual_ **agent**!",
   "id": "a51d3fa5b96fce78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### We begin by defining our \"tools\"\n",
    "Tools can be anything from internal / external APIs, logic within the app, databases lookups, etc."
   ],
   "id": "178a163c72112022"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def iceland_vacation_suggestion(topic: Literal['cafes', 'volcanoes', 'activities', 'other']) -> str:\n",
    "    \"\"\"Suggest a vacation spot in Iceland based on the topic. If the user doesn't state a topic, use the topic 'other'.\n",
    "\n",
    "    Args:\n",
    "        topic: The topic of interest. Must be one of 'cafes', 'volcanoes', 'activities', or 'other'.\n",
    "    \"\"\"\n",
    "    print(f\"--- iceland_vacation_suggestion called with {topic} ---\")\n",
    "\n",
    "    if topic == \"cafes\":\n",
    "        return \"Kaffibarinn\"\n",
    "    elif topic == \"volcanoes\":\n",
    "        return \"Fagradalsfjall\"\n",
    "    elif topic == \"activities\":\n",
    "        return \"Inside the Volcano\"\n",
    "    else:\n",
    "        return \"Harpa\"\n",
    "\n",
    "def iceland_vacation_spot_to_avoid(topic: Literal['cafes', 'volcanoes', 'activities', 'other']) -> str:\n",
    "    \"\"\"Suggest a vacation spot to avoid in Iceland, based on the topic. If the user doesn't state a topic, use the topic 'other'. 'other-.\n",
    "\n",
    "    Args:\n",
    "        topic: The topic of interest. Must be one of 'cafes', 'volcanoes', 'activities', or 'other'.\n",
    "    \"\"\"\n",
    "    print(f\"iceland_vacation_spots_to_avoid called with {topic}\")\n",
    "\n",
    "    if topic == \"cafes\":\n",
    "        return \"Cafe Babalu\"\n",
    "    elif topic == \"volcanoes\":\n",
    "        return \"Sundhnúkagígar / Grindavík\"\n",
    "    elif topic == \"activities\":\n",
    "        return \"Blue Lagoon\"\n",
    "    else:\n",
    "        return \"Aluminium smelters\""
   ],
   "id": "1dc3d71fb046590a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Next, we need to let the LLM know about our tools\n",
    "\n",
    "Some things to note:\n",
    "1. We bind the tools to the LLM, that is to say, we define the schema our tools and pass it to the LLM so it knows how to call them. The function `bind_tools` is a helper method that turns a list of functions into a **[JSON schema](http://json-schema.org)** that the LLM can understand.\n",
    "2. We set `parallel_tool_calls=False` to ensure that the tools are called sequentially. This is important when the tools have side effects or need to be called in a specific order. And in this case, it make the example a bit clearer."
   ],
   "id": "687ddf8cbf2563c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tools = [iceland_vacation_suggestion, iceland_vacation_spot_to_avoid]\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
   ],
   "id": "20059a3caf2538f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Now, we define our \"assistant\" node\n",
    "\n",
    "This time, we'll simply use a simple function to define our node.\n",
    "Note, that this time, we use the predefined **`MessagesState`** instead of defining our own state object. MessageState is a simple state object with a single key, `messages`, which is a list of `AnyMessage` (base class to all message types) objects."
   ],
   "id": "76b1a47bf40bae6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with tourist information assistance about Iceland.\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ],
   "id": "b0fee2c50dc4af6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### We can now build our graph\n",
    "\n",
    "Two things to note below:\n",
    "1. We use the predefined **`ToolNode`** for our tool calling node. This takes care of executing the actual tool/function based upon information in the LLM response about a tool call.\n",
    "2. We use the predefined **`tools_condition`** for our conditional edge. This will route the control flow to the tool calling node if the LLM returns information about a tool call in its response.\n"
   ],
   "id": "c2724809c3676f7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "# NOTE: Here we use the predefined tools_condition for our conditional edge\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ],
   "id": "8d8256723890dbfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let's test it out!",
   "id": "1a08beb2c81cce61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages = [HumanMessage(content=\"Hi! I'd like do a cool activity in Iceland!\")]\n",
    "#messages = [HumanMessage(content=\"Hi! I'm visiting Iceland next year and would like to do something fun and visit a volcano!\")]\n",
    "#messages = [HumanMessage(content=\"Can you suggest a good café I should go to when I visit Iceland? And is there any place I should avoid?\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ],
   "id": "ae6b07318758f693",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
